{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a29e63-88a5-4457-aea9-85735a11debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "from models import build_model\n",
    "from datasets import build_dataset\n",
    "from engine import evaluate, train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cee1ee-2c58-4b96-9231-b80149b5341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = torch.load('/home/marufm/intr-projects/INTR/output/fish_trained/output_sub/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a778b79f-97a5-4074-9059-94bbf2917904",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = torch.load('/home/marufm/intr-projects/INTR/output/fish_trained/output_sub/checkpoint0139.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb48fb4-0d60-4212-a3f3-0faf4c33b6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(m1['model']['backbone.0.body.layer4.2.bn3.weight'] != m2['model']['backbone.0.body.layer4.2.bn3.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb3ceab-565b-4ba0-8a06-d9676a48119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4671ae1c-5028-41de-917a-57217c6a25d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c8b349-e357-4151-9ff9-6e961dc69ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1950074-d4af-40c6-a5bf-4aa21b1036a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29c7a5-1be1-46d1-88c2-300dee8c2020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f8d5e5-af77-4dfe-bc20-d1df68353e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backbone='resnet50', batch_size=12, clip_max_norm=0.1, dataset_name='cub', dataset_path='/home/marufm/intr-projects/INTR/datasets', dec_layers=6, device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', dropout=0.1, enc_layers=6, epochs=140, eval=False, finetune='/home/marufm/intr-projects/INTR/checkpoints/detr-r50-e632da11.pth', hidden_dim=256, lr=0.0001, lr_backbone=1e-05, lr_drop=80, lr_scheduler='StepLR', min_lr=1e-06, nheads=8, noise_frac=0.1, num_queries=190, num_workers=2, output_dir='output', output_sub_dir='output_sub', position_embedding='sine', pre_norm=False, resume='', seed=42, start_epoch=0, test='val', weight_decay=1e-06, world_size=1)\n"
     ]
    }
   ],
   "source": [
    "args = torch.load('args_train.pt')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eb9b758-e28c-4410-a30b-a473032ab27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "git:\n",
      "  sha: 478f94b3ad77eda1997d037a52b3b1ca400d8997, status: has uncommited changes, branch: main\n",
      "\n",
      "Namespace(backbone='resnet50', batch_size=12, clip_max_norm=0.1, dataset_name='cub', dataset_path='/home/marufm/intr-projects/INTR/datasets', dec_layers=6, device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dropout=0.1, enc_layers=6, epochs=140, eval=False, finetune='/home/marufm/intr-projects/INTR/checkpoints/detr-r50-e632da11.pth', hidden_dim=256, lr=0.0001, lr_backbone=1e-05, lr_drop=80, lr_scheduler='StepLR', min_lr=1e-06, nheads=8, noise_frac=0.1, num_queries=190, num_workers=2, output_dir='output', output_sub_dir='output_sub', position_embedding='sine', pre_norm=False, resume='', seed=42, start_epoch=0, test='val', weight_decay=1e-06, world_size=1)\n"
     ]
    }
   ],
   "source": [
    "utils.init_distributed_mode(args)\n",
    "print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9359e2c1-0b3b-4210-950c-5f9a00d03476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30802040-fd24-475e-a216-f3c99556d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 --master_port 12345 --use_env main.py --finetune /home/marufm/intr-projects/INTR/checkpoints/detr-r50-e632da11.pth --dataset_path /home/marufm/intr-projects/INTR/datasets --dataset_name cub --num_queries 190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31468e22-dc6f-4290-8b85-b9371365c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 --master_port 12345 --use_env main.py --finetune /home/marufm/intr-projects/INTR/checkpoints/detr-r50-e632da11.pth --dataset_path /home/marufm/intr-projects/INTR/datasets --dataset_name butterfly --num_queries 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d1a99-2151-4829-af8e-cd170a2eebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 --master_port 12345 --use_env main.py --finetune /home/marufm/intr-projects/INTR/checkpoints/detr-r50-e632da11.pth --dataset_path /home/marufm/intr-projects/INTR/datasets --dataset_name fish --num_queries 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd23edb-d5c0-436d-98a0-f4373a3695f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d3479f1-940a-4e12-b3fc-fc2859c172b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marufm/miniconda/envs/intr/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/marufm/miniconda/envs/intr/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "model, criterion= build_model(args)\n",
    "model.to(device)\n",
    "model_without_ddp = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40dd01f1-03bc-4137-b00b-55f8905508ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 41171969\n"
     ]
    }
   ],
   "source": [
    "if args.distributed:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "    ## for 2-phase training\n",
    "    # model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=True) \n",
    "    model_without_ddp = model.module\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "\n",
    "param_dicts = [\n",
    "    {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "        \"lr\": args.lr_backbone,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dd2c7a0-427e-42b0-88ed-f904afe508bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6881598-60ac-467a-b5b3-f2703e8d397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.lr_scheduler==\"StepLR\":\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                                  weight_decay=args.weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "if args.lr_scheduler==\"CosineAnnealingLR\":\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr, \n",
    "                                weight_decay=args.weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=args.min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "162b04b7-818a-42ac-8bcc-039b1bb13a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = build_dataset(image_set='train', args=args)\n",
    "dataset_val = build_dataset(image_set=args.test, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b72654b-ff5e-4454-a126-683871388c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CreateImageFolder\n",
       "    Number of datapoints: 5695\n",
       "    Root location: /home/marufm/intr-projects/INTR/datasets/cub/train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               <datasets.transforms.RandomHorizontalFlip object at 0x7f1b00e09bb0>\n",
       "               <datasets.transforms.RandomSelect object at 0x7f1b05b7ebb0>\n",
       "               Compose(\n",
       "               <datasets.transforms.ToTensor object at 0x7f1b05c63d00>\n",
       "               <datasets.transforms.Normalize object at 0x7f1b00e09a30>\n",
       "           )\n",
       "           )"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67c286d5-08ec-432e-9f26-e3b86dcc0560",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "    sampler_train = DistributedSampler(dataset_train)\n",
    "    sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
    "else:\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                               collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                             drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6ba0aef-f88a-47d9-94e6-84b4a063dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch: [0]  [  0/474]  eta: 0:06:28  lr: 0.000100  loss: 5.5047 (5.5047)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.8201  data: 0.3794  max mem: 11888\n",
      "Epoch: [0]  [ 10/474]  eta: 0:03:24  lr: 0.000100  loss: 5.3324 (5.3465)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.7576)  time: 0.4402  data: 0.0472  max mem: 15618\n",
      "Epoch: [0]  [ 20/474]  eta: 0:03:13  lr: 0.000100  loss: 5.3402 (5.3553)  acc1: 0.0000 (0.3968)  acc5: 0.0000 (2.7778)  time: 0.4075  data: 0.0140  max mem: 17136\n",
      "Epoch: [0]  [ 30/474]  eta: 0:03:10  lr: 0.000100  loss: 5.3585 (5.3529)  acc1: 0.0000 (0.2688)  acc5: 0.0000 (3.2258)  time: 0.4229  data: 0.0142  max mem: 21137\n",
      "Epoch: [0]  [ 40/474]  eta: 0:03:02  lr: 0.000100  loss: 5.3608 (5.3479)  acc1: 0.0000 (0.2033)  acc5: 0.0000 (2.6423)  time: 0.4139  data: 0.0140  max mem: 21137\n",
      "Epoch: [0]  [ 50/474]  eta: 0:03:01  lr: 0.000100  loss: 5.3518 (5.3479)  acc1: 0.0000 (0.1634)  acc5: 0.0000 (2.4510)  time: 0.4286  data: 0.0140  max mem: 21137\n",
      "Epoch: [0]  [ 60/474]  eta: 0:02:55  lr: 0.000100  loss: 5.3388 (5.3400)  acc1: 0.0000 (0.1366)  acc5: 0.0000 (2.3224)  time: 0.4269  data: 0.0140  max mem: 21137\n",
      "Epoch: [0]  [ 70/474]  eta: 0:02:51  lr: 0.000100  loss: 5.3105 (5.3346)  acc1: 0.0000 (0.1174)  acc5: 0.0000 (2.4648)  time: 0.4091  data: 0.0140  max mem: 21137\n",
      "Epoch: [0]  [ 80/474]  eta: 0:02:46  lr: 0.000100  loss: 5.3032 (5.3321)  acc1: 0.0000 (0.2058)  acc5: 0.0000 (2.4691)  time: 0.4269  data: 0.0144  max mem: 21137\n",
      "Epoch: [0]  [ 90/474]  eta: 0:02:41  lr: 0.000100  loss: 5.3296 (5.3323)  acc1: 0.0000 (0.1832)  acc5: 0.0000 (2.5641)  time: 0.4161  data: 0.0140  max mem: 21137\n",
      "Epoch: [0]  [100/474]  eta: 0:02:38  lr: 0.000100  loss: 5.3257 (5.3297)  acc1: 0.0000 (0.3300)  acc5: 0.0000 (2.7228)  time: 0.4258  data: 0.0143  max mem: 21137\n",
      "Epoch: [0]  [110/474]  eta: 0:02:34  lr: 0.000100  loss: 5.3155 (5.3276)  acc1: 0.0000 (0.3003)  acc5: 0.0000 (2.7027)  time: 0.4318  data: 0.0145  max mem: 21137\n",
      "Epoch: [0]  [120/474]  eta: 0:02:30  lr: 0.000100  loss: 5.3203 (5.3288)  acc1: 0.0000 (0.2755)  acc5: 0.0000 (2.5482)  time: 0.4323  data: 0.0143  max mem: 21137\n",
      "Epoch: [0]  [130/474]  eta: 0:02:25  lr: 0.000100  loss: 5.3043 (5.3278)  acc1: 0.0000 (0.3817)  acc5: 0.0000 (2.6718)  time: 0.4260  data: 0.0144  max mem: 21137\n",
      "Epoch: [0]  [140/474]  eta: 0:02:21  lr: 0.000100  loss: 5.2910 (5.3254)  acc1: 0.0000 (0.4137)  acc5: 0.0000 (2.6596)  time: 0.4030  data: 0.0140  max mem: 21137\n",
      "Epoch: [0]  [150/474]  eta: 0:02:16  lr: 0.000100  loss: 5.3030 (5.3268)  acc1: 0.0000 (0.3863)  acc5: 0.0000 (2.5938)  time: 0.4152  data: 0.0140  max mem: 21137\n",
      "Epoch: [0]  [160/474]  eta: 0:02:12  lr: 0.000100  loss: 5.3165 (5.3263)  acc1: 0.0000 (0.4141)  acc5: 0.0000 (2.6398)  time: 0.4218  data: 0.0142  max mem: 21137\n",
      "Epoch: [0]  [170/474]  eta: 0:02:08  lr: 0.000100  loss: 5.3555 (5.3283)  acc1: 0.0000 (0.3899)  acc5: 0.0000 (2.6316)  time: 0.4229  data: 0.0143  max mem: 21137\n",
      "Epoch: [0]  [180/474]  eta: 0:02:04  lr: 0.000100  loss: 5.3555 (5.3287)  acc1: 0.0000 (0.3683)  acc5: 0.0000 (2.6243)  time: 0.4308  data: 0.0143  max mem: 21137\n",
      "Epoch: [0]  [190/474]  eta: 0:02:00  lr: 0.000100  loss: 5.3043 (5.3276)  acc1: 0.0000 (0.4799)  acc5: 0.0000 (2.7051)  time: 0.4275  data: 0.0142  max mem: 21137\n",
      "Epoch: [0]  [200/474]  eta: 0:01:55  lr: 0.000100  loss: 5.3043 (5.3279)  acc1: 0.0000 (0.4561)  acc5: 0.0000 (2.6949)  time: 0.4220  data: 0.0142  max mem: 21137\n",
      "Epoch: [0]  [210/474]  eta: 0:01:51  lr: 0.000100  loss: 5.3108 (5.3259)  acc1: 0.0000 (0.4739)  acc5: 0.0000 (2.7251)  time: 0.4091  data: 0.0140  max mem: 21137\n",
      "Epoch: [0]  [220/474]  eta: 0:01:47  lr: 0.000100  loss: 5.2930 (5.3261)  acc1: 0.0000 (0.4525)  acc5: 0.0000 (2.6772)  time: 0.4048  data: 0.0139  max mem: 21137\n",
      "Epoch: [0]  [230/474]  eta: 0:01:42  lr: 0.000100  loss: 5.3210 (5.3258)  acc1: 0.0000 (0.4690)  acc5: 0.0000 (2.6696)  time: 0.4205  data: 0.0143  max mem: 21137\n",
      "Epoch: [0]  [240/474]  eta: 0:01:38  lr: 0.000100  loss: 5.3192 (5.3247)  acc1: 0.0000 (0.4495)  acc5: 0.0000 (2.5588)  time: 0.4392  data: 0.0148  max mem: 21137\n",
      "Epoch: [0]  [250/474]  eta: 0:01:34  lr: 0.000100  loss: 5.3226 (5.3255)  acc1: 0.0000 (0.4316)  acc5: 0.0000 (2.4900)  time: 0.4341  data: 0.0147  max mem: 21137\n",
      "Epoch: [0]  [260/474]  eta: 0:01:30  lr: 0.000100  loss: 5.3239 (5.3241)  acc1: 0.0000 (0.4470)  acc5: 0.0000 (2.5862)  time: 0.4356  data: 0.0147  max mem: 21137\n",
      "Epoch: [0]  [270/474]  eta: 0:01:26  lr: 0.000100  loss: 5.3099 (5.3240)  acc1: 0.0000 (0.4305)  acc5: 0.0000 (2.5523)  time: 0.4464  data: 0.0150  max mem: 21137\n",
      "Epoch: [0]  [280/474]  eta: 0:01:22  lr: 0.000100  loss: 5.3077 (5.3234)  acc1: 0.0000 (0.4152)  acc5: 0.0000 (2.5504)  time: 0.4336  data: 0.0146  max mem: 21137\n",
      "Epoch: [0]  [290/474]  eta: 0:01:17  lr: 0.000100  loss: 5.2976 (5.3229)  acc1: 0.0000 (0.4009)  acc5: 0.0000 (2.5773)  time: 0.4142  data: 0.0143  max mem: 21137\n",
      "Epoch: [0]  [300/474]  eta: 0:01:13  lr: 0.000100  loss: 5.3052 (5.3225)  acc1: 0.0000 (0.3876)  acc5: 0.0000 (2.5748)  time: 0.4173  data: 0.0145  max mem: 21330\n",
      "Epoch: [0]  [310/474]  eta: 0:01:09  lr: 0.000100  loss: 5.2968 (5.3215)  acc1: 0.0000 (0.4287)  acc5: 0.0000 (2.6527)  time: 0.4427  data: 0.0147  max mem: 23397\n",
      "Epoch: [0]  [320/474]  eta: 0:01:05  lr: 0.000100  loss: 5.3039 (5.3206)  acc1: 0.0000 (0.4154)  acc5: 0.0000 (2.6739)  time: 0.4341  data: 0.0147  max mem: 23397\n",
      "Epoch: [0]  [330/474]  eta: 0:01:01  lr: 0.000100  loss: 5.3039 (5.3199)  acc1: 0.0000 (0.4028)  acc5: 0.0000 (2.6687)  time: 0.4038  data: 0.0143  max mem: 23397\n",
      "Epoch: [0]  [340/474]  eta: 0:00:56  lr: 0.000100  loss: 5.3001 (5.3189)  acc1: 0.0000 (0.3910)  acc5: 0.0000 (2.6882)  time: 0.3909  data: 0.0139  max mem: 23397\n",
      "Epoch: [0]  [350/474]  eta: 0:00:52  lr: 0.000100  loss: 5.3007 (5.3188)  acc1: 0.0000 (0.3799)  acc5: 0.0000 (2.6591)  time: 0.4052  data: 0.0142  max mem: 23397\n",
      "Epoch: [0]  [360/474]  eta: 0:00:48  lr: 0.000100  loss: 5.3238 (5.3192)  acc1: 0.0000 (0.3693)  acc5: 0.0000 (2.5854)  time: 0.4163  data: 0.0147  max mem: 23397\n",
      "Epoch: [0]  [370/474]  eta: 0:00:43  lr: 0.000100  loss: 5.3169 (5.3186)  acc1: 0.0000 (0.3819)  acc5: 0.0000 (2.6730)  time: 0.4174  data: 0.0146  max mem: 23397\n",
      "Epoch: [0]  [380/474]  eta: 0:00:39  lr: 0.000100  loss: 5.2890 (5.3184)  acc1: 0.0000 (0.3718)  acc5: 0.0000 (2.6903)  time: 0.4264  data: 0.0145  max mem: 23397\n",
      "Epoch: [0]  [390/474]  eta: 0:00:35  lr: 0.000100  loss: 5.2970 (5.3180)  acc1: 0.0000 (0.3836)  acc5: 0.0000 (2.7280)  time: 0.4384  data: 0.0149  max mem: 23397\n",
      "Epoch: [0]  [400/474]  eta: 0:00:31  lr: 0.000100  loss: 5.3082 (5.3180)  acc1: 0.0000 (0.3948)  acc5: 0.0000 (2.7224)  time: 0.4348  data: 0.0150  max mem: 23397\n",
      "Epoch: [0]  [410/474]  eta: 0:00:27  lr: 0.000100  loss: 5.3427 (5.3192)  acc1: 0.0000 (0.4258)  acc5: 0.0000 (2.7170)  time: 0.4270  data: 0.0147  max mem: 23397\n",
      "Epoch: [0]  [420/474]  eta: 0:00:22  lr: 0.000100  loss: 5.3390 (5.3189)  acc1: 0.0000 (0.4355)  acc5: 0.0000 (2.7316)  time: 0.4415  data: 0.0149  max mem: 23397\n",
      "Epoch: [0]  [430/474]  eta: 0:00:18  lr: 0.000100  loss: 5.2977 (5.3188)  acc1: 0.0000 (0.4447)  acc5: 0.0000 (2.6875)  time: 0.4728  data: 0.0158  max mem: 24474\n",
      "Epoch: [0]  [440/474]  eta: 0:00:14  lr: 0.000100  loss: 5.3038 (5.3187)  acc1: 0.0000 (0.4346)  acc5: 0.0000 (2.6455)  time: 0.4620  data: 0.0156  max mem: 24474\n",
      "Epoch: [0]  [450/474]  eta: 0:00:10  lr: 0.000100  loss: 5.3038 (5.3189)  acc1: 0.0000 (0.4250)  acc5: 0.0000 (2.6608)  time: 0.4474  data: 0.0150  max mem: 24474\n",
      "Epoch: [0]  [460/474]  eta: 0:00:05  lr: 0.000100  loss: 5.3110 (5.3190)  acc1: 0.0000 (0.4338)  acc5: 0.0000 (2.6573)  time: 0.4476  data: 0.0151  max mem: 24474\n",
      "Epoch: [0]  [470/474]  eta: 0:00:01  lr: 0.000100  loss: 5.3110 (5.3188)  acc1: 0.0000 (0.4246)  acc5: 0.0000 (2.6716)  time: 0.4522  data: 0.0150  max mem: 25360\n",
      "Epoch: [0]  [473/474]  eta: 0:00:00  lr: 0.000100  loss: 5.3048 (5.3187)  acc1: 0.0000 (0.4219)  acc5: 0.0000 (2.6547)  time: 0.4453  data: 0.0149  max mem: 25360\n",
      "Epoch: [0] Total time: 0:03:22 (0.4277 s / it)\n",
      "Averaged stats: lr: 0.000100  loss: 5.3048 (5.3187)  acc1: 0.0000 (0.4219)  acc5: 0.0000 (2.6547)\n",
      "Test:  [  0/460]  eta: 0:03:50  loss: 5.2862 (5.2862)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.5016  data: 0.3497  max mem: 25360\n",
      "Test:  [ 10/460]  eta: 0:01:27  loss: 5.3037 (5.2963)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.1955  data: 0.0438  max mem: 25360\n",
      "Test:  [ 20/460]  eta: 0:01:19  loss: 5.3001 (5.2707)  acc1: 0.0000 (4.7619)  acc5: 0.0000 (11.1111)  time: 0.1651  data: 0.0133  max mem: 25360\n",
      "Test:  [ 30/460]  eta: 0:01:15  loss: 5.3009 (5.2837)  acc1: 0.0000 (3.2258)  acc5: 0.0000 (7.5269)  time: 0.1654  data: 0.0134  max mem: 25360\n",
      "Test:  [ 40/460]  eta: 0:01:12  loss: 5.3009 (5.2869)  acc1: 0.0000 (2.4390)  acc5: 0.0000 (6.9106)  time: 0.1654  data: 0.0134  max mem: 25360\n",
      "Test:  [ 50/460]  eta: 0:01:10  loss: 5.3046 (5.2942)  acc1: 0.0000 (1.9608)  acc5: 0.0000 (5.7190)  time: 0.1653  data: 0.0133  max mem: 25360\n",
      "Test:  [ 60/460]  eta: 0:01:08  loss: 5.3058 (5.2970)  acc1: 0.0000 (1.6393)  acc5: 0.0000 (4.9180)  time: 0.1653  data: 0.0134  max mem: 25360\n",
      "Test:  [ 70/460]  eta: 0:01:06  loss: 5.2885 (5.2941)  acc1: 0.0000 (1.4085)  acc5: 0.0000 (4.2254)  time: 0.1652  data: 0.0134  max mem: 25360\n",
      "Test:  [ 80/460]  eta: 0:01:04  loss: 5.2716 (5.2909)  acc1: 0.0000 (1.4403)  acc5: 0.0000 (4.5267)  time: 0.1653  data: 0.0135  max mem: 25360\n",
      "Test:  [ 90/460]  eta: 0:01:02  loss: 5.2797 (5.2907)  acc1: 0.0000 (1.2821)  acc5: 0.0000 (4.3040)  time: 0.1654  data: 0.0136  max mem: 25360\n",
      "Test:  [100/460]  eta: 0:01:00  loss: 5.2875 (5.2908)  acc1: 0.0000 (1.1551)  acc5: 0.0000 (3.9604)  time: 0.1654  data: 0.0136  max mem: 25360\n",
      "Test:  [110/460]  eta: 0:00:58  loss: 5.2933 (5.2931)  acc1: 0.0000 (1.0511)  acc5: 0.0000 (3.6036)  time: 0.1657  data: 0.0138  max mem: 25360\n",
      "Test:  [120/460]  eta: 0:00:57  loss: 5.2926 (5.2918)  acc1: 0.0000 (1.1019)  acc5: 0.0000 (4.2700)  time: 0.1655  data: 0.0135  max mem: 25360\n",
      "Test:  [130/460]  eta: 0:00:55  loss: 5.2449 (5.2870)  acc1: 0.0000 (1.1450)  acc5: 8.3333 (4.5165)  time: 0.1654  data: 0.0134  max mem: 25360\n",
      "Test:  [140/460]  eta: 0:00:53  loss: 5.2582 (5.2877)  acc1: 0.0000 (1.0638)  acc5: 0.0000 (4.3735)  time: 0.1656  data: 0.0136  max mem: 25360\n",
      "Test:  [150/460]  eta: 0:00:51  loss: 5.2702 (5.2855)  acc1: 0.0000 (0.9934)  acc5: 0.0000 (4.0839)  time: 0.1655  data: 0.0136  max mem: 25360\n",
      "Test:  [160/460]  eta: 0:00:50  loss: 5.2669 (5.2848)  acc1: 0.0000 (1.1905)  acc5: 0.0000 (4.2961)  time: 0.1656  data: 0.0136  max mem: 25360\n",
      "Test:  [170/460]  eta: 0:00:48  loss: 5.2841 (5.2858)  acc1: 0.0000 (1.1209)  acc5: 0.0000 (4.0448)  time: 0.1656  data: 0.0137  max mem: 25360\n",
      "Test:  [180/460]  eta: 0:00:46  loss: 5.3140 (5.2879)  acc1: 0.0000 (1.0589)  acc5: 0.0000 (3.8214)  time: 0.1656  data: 0.0137  max mem: 25360\n",
      "Test:  [190/460]  eta: 0:00:45  loss: 5.2628 (5.2862)  acc1: 0.0000 (1.0471)  acc5: 0.0000 (4.0140)  time: 0.1657  data: 0.0137  max mem: 25360\n",
      "Test:  [200/460]  eta: 0:00:43  loss: 5.2577 (5.2853)  acc1: 0.0000 (0.9950)  acc5: 0.0000 (3.9386)  time: 0.1657  data: 0.0137  max mem: 25360\n",
      "Test:  [210/460]  eta: 0:00:41  loss: 5.2805 (5.2846)  acc1: 0.0000 (0.9479)  acc5: 0.0000 (3.7520)  time: 0.1657  data: 0.0137  max mem: 25360\n",
      "Test:  [220/460]  eta: 0:00:40  loss: 5.3193 (5.2872)  acc1: 0.0000 (0.9050)  acc5: 0.0000 (3.5822)  time: 0.1657  data: 0.0137  max mem: 25360\n",
      "Test:  [230/460]  eta: 0:00:38  loss: 5.3464 (5.2897)  acc1: 0.0000 (0.8658)  acc5: 0.0000 (3.4271)  time: 0.1657  data: 0.0138  max mem: 25360\n",
      "Test:  [240/460]  eta: 0:00:36  loss: 5.3399 (5.2914)  acc1: 0.0000 (0.8299)  acc5: 0.0000 (3.2849)  time: 0.1658  data: 0.0138  max mem: 25360\n",
      "Test:  [250/460]  eta: 0:00:35  loss: 5.3226 (5.2920)  acc1: 0.0000 (0.7968)  acc5: 0.0000 (3.1541)  time: 0.1657  data: 0.0137  max mem: 25360\n",
      "Test:  [260/460]  eta: 0:00:33  loss: 5.2848 (5.2912)  acc1: 0.0000 (0.8621)  acc5: 0.0000 (3.5441)  time: 0.1659  data: 0.0138  max mem: 25360\n",
      "Test:  [270/460]  eta: 0:00:31  loss: 5.2775 (5.2912)  acc1: 0.0000 (0.8303)  acc5: 0.0000 (3.4133)  time: 0.1655  data: 0.0135  max mem: 25360\n",
      "Test:  [280/460]  eta: 0:00:30  loss: 5.2795 (5.2907)  acc1: 0.0000 (0.8304)  acc5: 0.0000 (3.3511)  time: 0.1656  data: 0.0136  max mem: 25360\n",
      "Test:  [290/460]  eta: 0:00:28  loss: 5.2812 (5.2906)  acc1: 0.0000 (0.8018)  acc5: 0.0000 (3.2646)  time: 0.1660  data: 0.0140  max mem: 25360\n",
      "Test:  [300/460]  eta: 0:00:26  loss: 5.2860 (5.2907)  acc1: 0.0000 (0.8306)  acc5: 0.0000 (3.4884)  time: 0.1659  data: 0.0139  max mem: 25360\n",
      "Test:  [310/460]  eta: 0:00:24  loss: 5.2784 (5.2908)  acc1: 0.0000 (0.8039)  acc5: 0.0000 (3.3762)  time: 0.1660  data: 0.0139  max mem: 25360\n",
      "Test:  [320/460]  eta: 0:00:23  loss: 5.2630 (5.2894)  acc1: 0.0000 (0.7788)  acc5: 0.0000 (3.3229)  time: 0.1659  data: 0.0139  max mem: 25360\n",
      "Test:  [330/460]  eta: 0:00:21  loss: 5.2567 (5.2884)  acc1: 0.0000 (0.7805)  acc5: 0.0000 (3.3988)  time: 0.1656  data: 0.0137  max mem: 25360\n",
      "Test:  [340/460]  eta: 0:00:19  loss: 5.2788 (5.2883)  acc1: 0.0000 (0.7576)  acc5: 0.0000 (3.4213)  time: 0.1655  data: 0.0137  max mem: 25360\n",
      "Test:  [350/460]  eta: 0:00:18  loss: 5.2801 (5.2883)  acc1: 0.0000 (0.7360)  acc5: 0.0000 (3.3238)  time: 0.1657  data: 0.0138  max mem: 25360\n",
      "Test:  [360/460]  eta: 0:00:16  loss: 5.2844 (5.2887)  acc1: 0.0000 (0.7618)  acc5: 0.0000 (3.4164)  time: 0.1660  data: 0.0140  max mem: 25360\n",
      "Test:  [370/460]  eta: 0:00:14  loss: 5.2873 (5.2885)  acc1: 0.0000 (0.7412)  acc5: 0.0000 (3.3243)  time: 0.1659  data: 0.0139  max mem: 25360\n",
      "Test:  [380/460]  eta: 0:00:13  loss: 5.2591 (5.2882)  acc1: 0.0000 (0.7218)  acc5: 0.0000 (3.3246)  time: 0.1659  data: 0.0139  max mem: 25360\n",
      "Test:  [390/460]  eta: 0:00:11  loss: 5.2599 (5.2881)  acc1: 0.0000 (0.7033)  acc5: 0.0000 (3.2822)  time: 0.1659  data: 0.0139  max mem: 25360\n",
      "Test:  [400/460]  eta: 0:00:09  loss: 5.2700 (5.2877)  acc1: 0.0000 (0.6858)  acc5: 0.0000 (3.4497)  time: 0.1659  data: 0.0139  max mem: 25360\n",
      "Test:  [410/460]  eta: 0:00:08  loss: 5.2666 (5.2870)  acc1: 0.0000 (0.6691)  acc5: 0.0000 (3.4266)  time: 0.1658  data: 0.0139  max mem: 25360\n",
      "Test:  [420/460]  eta: 0:00:06  loss: 5.2887 (5.2877)  acc1: 0.0000 (0.6532)  acc5: 0.0000 (3.3452)  time: 0.1658  data: 0.0139  max mem: 25360\n",
      "Test:  [430/460]  eta: 0:00:04  loss: 5.3214 (5.2880)  acc1: 0.0000 (0.6574)  acc5: 0.0000 (3.3063)  time: 0.1659  data: 0.0139  max mem: 25360\n",
      "Test:  [440/460]  eta: 0:00:03  loss: 5.2922 (5.2877)  acc1: 0.0000 (0.6803)  acc5: 0.0000 (3.3447)  time: 0.1658  data: 0.0138  max mem: 25360\n",
      "Test:  [450/460]  eta: 0:00:01  loss: 5.2735 (5.2873)  acc1: 0.0000 (0.6652)  acc5: 0.0000 (3.2705)  time: 0.1656  data: 0.0137  max mem: 25360\n",
      "Test:  [459/460]  eta: 0:00:00  loss: 5.2778 (5.2879)  acc1: 0.0000 (0.6522)  acc5: 0.0000 (3.2065)  time: 0.1596  data: 0.0132  max mem: 25360\n",
      "Test: Total time: 0:01:16 (0.1663 s / it)\n",
      "Averaged stats: loss: 5.2778 (5.2879)  acc1: 0.0000 (0.6522)  acc5: 0.0000 (3.2065)\n",
      "Epoch: [1]  [  0/474]  eta: 0:07:05  lr: 0.000100  loss: 5.3000 (5.3000)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.8984  data: 0.3337  max mem: 25360\n",
      "Epoch: [1]  [ 10/474]  eta: 0:03:34  lr: 0.000100  loss: 5.2957 (5.2994)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (3.7879)  time: 0.4616  data: 0.0429  max mem: 25360\n",
      "Epoch: [1]  [ 20/474]  eta: 0:03:30  lr: 0.000100  loss: 5.2957 (5.3048)  acc1: 0.0000 (0.3968)  acc5: 0.0000 (3.1746)  time: 0.4410  data: 0.0144  max mem: 26039\n",
      "Epoch: [1]  [ 30/474]  eta: 0:03:19  lr: 0.000100  loss: 5.2978 (5.2990)  acc1: 0.0000 (0.8065)  acc5: 0.0000 (3.7634)  time: 0.4413  data: 0.0145  max mem: 26039\n",
      "Epoch: [1]  [ 40/474]  eta: 0:03:12  lr: 0.000100  loss: 5.2897 (5.2938)  acc1: 0.0000 (1.0163)  acc5: 0.0000 (3.6585)  time: 0.4251  data: 0.0141  max mem: 26039\n",
      "Epoch: [1]  [ 50/474]  eta: 0:03:08  lr: 0.000100  loss: 5.2680 (5.2870)  acc1: 0.0000 (0.8170)  acc5: 0.0000 (3.4314)  time: 0.4357  data: 0.0143  max mem: 26039\n",
      "Epoch: [1]  [ 60/474]  eta: 0:02:59  lr: 0.000100  loss: 5.2841 (5.2883)  acc1: 0.0000 (0.6831)  acc5: 0.0000 (3.1421)  time: 0.4147  data: 0.0144  max mem: 26039\n",
      "Epoch: [1]  [ 70/474]  eta: 0:02:55  lr: 0.000100  loss: 5.2873 (5.2892)  acc1: 0.0000 (0.7042)  acc5: 0.0000 (3.1690)  time: 0.4137  data: 0.0143  max mem: 26039\n",
      "Epoch: [1]  [ 80/474]  eta: 0:02:51  lr: 0.000100  loss: 5.2622 (5.2823)  acc1: 0.0000 (0.6173)  acc5: 0.0000 (3.6008)  time: 0.4328  data: 0.0144  max mem: 26039\n",
      "Epoch: [1]  [ 90/474]  eta: 0:02:46  lr: 0.000100  loss: 5.2539 (5.2791)  acc1: 0.0000 (0.6410)  acc5: 8.3333 (3.8462)  time: 0.4307  data: 0.0144  max mem: 26039\n",
      "Epoch: [1]  [100/474]  eta: 0:02:41  lr: 0.000100  loss: 5.2600 (5.2782)  acc1: 0.0000 (0.6601)  acc5: 0.0000 (3.9604)  time: 0.4267  data: 0.0143  max mem: 26039\n",
      "Epoch: [1]  [110/474]  eta: 0:02:37  lr: 0.000100  loss: 5.2822 (5.2807)  acc1: 0.0000 (0.6006)  acc5: 0.0000 (3.6787)  time: 0.4274  data: 0.0146  max mem: 26039\n",
      "Epoch: [1]  [120/474]  eta: 0:02:33  lr: 0.000100  loss: 5.2749 (5.2802)  acc1: 0.0000 (0.5510)  acc5: 0.0000 (3.5813)  time: 0.4433  data: 0.0151  max mem: 26039\n",
      "Epoch: [1]  [130/474]  eta: 0:02:29  lr: 0.000100  loss: 5.2919 (5.2824)  acc1: 0.0000 (0.5089)  acc5: 0.0000 (3.3715)  time: 0.4434  data: 0.0152  max mem: 26039\n",
      "Epoch: [1]  [140/474]  eta: 0:02:24  lr: 0.000100  loss: 5.2919 (5.2822)  acc1: 0.0000 (0.5319)  acc5: 0.0000 (3.4279)  time: 0.4199  data: 0.0147  max mem: 26039\n",
      "Epoch: [1]  [150/474]  eta: 0:02:19  lr: 0.000100  loss: 5.2625 (5.2782)  acc1: 0.0000 (0.5519)  acc5: 0.0000 (3.6424)  time: 0.4060  data: 0.0143  max mem: 26039\n",
      "Epoch: [1]  [160/474]  eta: 0:02:14  lr: 0.000100  loss: 5.2971 (5.2824)  acc1: 0.0000 (0.5694)  acc5: 0.0000 (3.5714)  time: 0.4124  data: 0.0143  max mem: 26039\n",
      "Epoch: [1]  [170/474]  eta: 0:02:11  lr: 0.000100  loss: 5.2971 (5.2812)  acc1: 0.0000 (0.6335)  acc5: 0.0000 (3.6062)  time: 0.4330  data: 0.0153  max mem: 26039\n",
      "Epoch: [1]  [180/474]  eta: 0:02:06  lr: 0.000100  loss: 5.2393 (5.2811)  acc1: 0.0000 (0.5985)  acc5: 0.0000 (3.7753)  time: 0.4409  data: 0.0156  max mem: 26039\n",
      "Epoch: [1]  [190/474]  eta: 0:02:02  lr: 0.000100  loss: 5.2392 (5.2805)  acc1: 0.0000 (0.5672)  acc5: 0.0000 (3.7522)  time: 0.4272  data: 0.0150  max mem: 26039\n",
      "Epoch: [1]  [200/474]  eta: 0:01:57  lr: 0.000100  loss: 5.2577 (5.2796)  acc1: 0.0000 (0.5804)  acc5: 0.0000 (3.8972)  time: 0.4183  data: 0.0148  max mem: 26039\n",
      "Epoch: [1]  [210/474]  eta: 0:01:53  lr: 0.000100  loss: 5.2598 (5.2798)  acc1: 0.0000 (0.5529)  acc5: 0.0000 (3.7915)  time: 0.4214  data: 0.0150  max mem: 26039\n",
      "Epoch: [1]  [220/474]  eta: 0:01:48  lr: 0.000100  loss: 5.2558 (5.2786)  acc1: 0.0000 (0.5279)  acc5: 0.0000 (3.8084)  time: 0.4097  data: 0.0147  max mem: 26039\n",
      "Epoch: [1]  [230/474]  eta: 0:01:44  lr: 0.000100  loss: 5.2825 (5.2809)  acc1: 0.0000 (0.5411)  acc5: 0.0000 (3.8600)  time: 0.4105  data: 0.0147  max mem: 26039\n",
      "Epoch: [1]  [240/474]  eta: 0:01:40  lr: 0.000100  loss: 5.2499 (5.2789)  acc1: 0.0000 (0.5878)  acc5: 8.3333 (3.9073)  time: 0.4299  data: 0.0154  max mem: 26039\n",
      "Epoch: [1]  [250/474]  eta: 0:01:36  lr: 0.000100  loss: 5.2485 (5.2788)  acc1: 0.0000 (0.5644)  acc5: 0.0000 (3.8181)  time: 0.4464  data: 0.0157  max mem: 26039\n",
      "Epoch: [1]  [260/474]  eta: 0:01:32  lr: 0.000100  loss: 5.2706 (5.2780)  acc1: 0.0000 (0.5428)  acc5: 0.0000 (3.7676)  time: 0.4634  data: 0.0166  max mem: 26039\n",
      "Epoch: [1]  [270/474]  eta: 0:01:27  lr: 0.000100  loss: 5.2474 (5.2769)  acc1: 0.0000 (0.5535)  acc5: 0.0000 (3.8745)  time: 0.4383  data: 0.0160  max mem: 26039\n",
      "Epoch: [1]  [280/474]  eta: 0:01:23  lr: 0.000100  loss: 5.2304 (5.2745)  acc1: 0.0000 (0.6524)  acc5: 8.3333 (3.9442)  time: 0.4370  data: 0.0160  max mem: 26039\n",
      "Epoch: [1]  [290/474]  eta: 0:01:19  lr: 0.000100  loss: 5.2809 (5.2766)  acc1: 0.0000 (0.6586)  acc5: 0.0000 (3.8660)  time: 0.4401  data: 0.0160  max mem: 26039\n",
      "Epoch: [1]  [300/474]  eta: 0:01:14  lr: 0.000100  loss: 5.2914 (5.2772)  acc1: 0.0000 (0.6368)  acc5: 0.0000 (3.8483)  time: 0.4253  data: 0.0156  max mem: 26039\n",
      "Epoch: [1]  [310/474]  eta: 0:01:10  lr: 0.000100  loss: 5.2801 (5.2769)  acc1: 0.0000 (0.6163)  acc5: 0.0000 (3.7781)  time: 0.4322  data: 0.0160  max mem: 26039\n",
      "Epoch: [1]  [320/474]  eta: 0:01:06  lr: 0.000100  loss: 5.2712 (5.2768)  acc1: 0.0000 (0.5971)  acc5: 0.0000 (3.7383)  time: 0.4218  data: 0.0156  max mem: 26039\n",
      "Epoch: [1]  [330/474]  eta: 0:01:01  lr: 0.000100  loss: 5.2263 (5.2755)  acc1: 0.0000 (0.6294)  acc5: 0.0000 (3.7261)  time: 0.4138  data: 0.0156  max mem: 26039\n",
      "Epoch: [1]  [340/474]  eta: 0:00:57  lr: 0.000100  loss: 5.2594 (5.2770)  acc1: 0.0000 (0.6354)  acc5: 0.0000 (3.7634)  time: 0.4231  data: 0.0158  max mem: 26039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/marufm/miniconda/envs/intr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/marufm/miniconda/envs/intr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/marufm/miniconda/envs/intr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/marufm/miniconda/envs/intr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdistributed:\n\u001b[1;32m     53\u001b[0m     sampler_train\u001b[38;5;241m.\u001b[39mset_epoch(epoch)\n\u001b[0;32m---> 54\u001b[0m train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_max_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39moutput_dir:\n",
      "File \u001b[0;32m~/intr-projects/INTR/engine.py:58\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch, max_norm)\u001b[0m\n\u001b[1;32m     56\u001b[0m losses\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_norm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     61\u001b[0m acc1, acc5, _ \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mclass_accuracy(outputs, targets, topk\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda/envs/intr/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py:76\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ((device, _), [grads]) \u001b[38;5;129;01min\u001b[39;00m grouped_grads\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m foreach) \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(grads, device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[0;32m---> 76\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_coef_clamped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#   We create output directories to store results\n",
    "output_dir = Path(args.output_dir)\n",
    "if not os.path.exists(os.path.join(output_dir, args.dataset_name)):\n",
    "    os.makedirs(os.path.join(output_dir, args.dataset_name), exist_ok=True)\n",
    "if not os.path.exists(os.path.join(output_dir, args.dataset_name, args.output_sub_dir)):\n",
    "    os.makedirs(os.path.join(output_dir, args.dataset_name, args.output_sub_dir), exist_ok=True)\n",
    "\n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model_without_ddp.load_state_dict(checkpoint['model'])\n",
    "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "if args.eval:\n",
    "    test_stats = evaluate(model, criterion, \n",
    "                            data_loader_val, device, args.output_dir)\n",
    "    if args.output_dir and utils.is_main_process():\n",
    "        with (output_dir / args.dataset_name / args.output_sub_dir/ \"log.txt\").open(\"a\") as f:\n",
    "            f.write(json.dumps(test_stats) + \"\\n\")\n",
    "    # return\n",
    "\n",
    "if args.finetune:\n",
    "    if args.finetune.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.finetune, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.finetune, map_location='cpu')\n",
    "    state_dict = checkpoint['model']\n",
    "    state_dict=utils.load_model(args, state_dict)\n",
    "    \n",
    "    model_without_ddp.load_state_dict(state_dict)\n",
    "\n",
    "    for param in model_without_ddp.parameters():\n",
    "        param.requires_grad = True\n",
    "    model_without_ddp.to(device)\n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "\n",
    "    ## for 2-phase training\n",
    "    # if epoch>=args.rm_freeze:\n",
    "    #     for param in model_without_ddp.transformer.encoder.parameters():\n",
    "    #         param.requires_grad = True\n",
    "\n",
    "    if args.distributed:\n",
    "        sampler_train.set_epoch(epoch)\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "        args.clip_max_norm)\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    if args.output_dir:\n",
    "        checkpoint_paths = [output_dir / args.dataset_name / args.output_sub_dir/ 'checkpoint.pth']\n",
    "\n",
    "        if (epoch + 1) % args.lr_drop == 0 or (epoch + 1)==args.epochs:\n",
    "            checkpoint_paths.append(output_dir / args.dataset_name / args.output_sub_dir / f'checkpoint{epoch:04}.pth')\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            utils.save_on_master({\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    test_stats = evaluate(\n",
    "        model, criterion,  data_loader_val, device, args.output_dir\n",
    "    )\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                 **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                 'epoch': epoch,\n",
    "                 'n_parameters': n_parameters}\n",
    "\n",
    "    if args.output_dir and utils.is_main_process():\n",
    "        with (output_dir / args.dataset_name / args.output_sub_dir/ \"log.txt\").open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce44f2-dfb2-43dd-9982-acf582c84719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intr",
   "language": "python",
   "name": "intr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
